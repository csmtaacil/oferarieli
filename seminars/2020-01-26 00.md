---
evtSpeaker: "Lior Kammah"
evtSpeaker_he: "ליאור קמה"
evtDate: 2020-01-28
evtTime: 1500
genderSpeak: ידבר
evtTitle: "Fully Understanding the Hashing-Trick"
evtAbstract: |
   Feature hashing,
   also known as the hashing trick, 
   introduced by Weinberger et al. (2009), 
   is one of the key techniques used in scaling-up machine learning algorithms.
   Loosely speaking, feature hashing uses a random hashing of the entries of an
   n-dimensional x in order to reduce the dimension of the data from n to m
   (where m<<n), 
   while approximately preserving the Euclidean norm.
   Weinberger et al. showed tail bounds on the norm of the new vector.
   Specifically they showed that if an n-dimensional vector x is
   sufficiently "balanced",
   and the target dimension m is sufficiently large,
   then the norm of the hashed vector approximates the norm of x with high probability.
   These bounds were later extended by Dasgupta et al. (2010) and
   most recently refined by Dahlgaard et al. (2017), however, 
   the true nature of the performance of this key technique, 
   and specifically the correct tradeoff between the pivotal parameters (
   how "balanced" x is, and what are the constraints on the error) remained
   an open question.
   We settle this question by giving tight asymptotic bounds on the exact 
   tradeoff between the central parameters,
   thus providing a complete understanding of the performance of feature hashing. 
   We complement the asymptotic bound with empirical data,
   which shows that the constants "hiding" in the asymptotic notation are, in fact,
   very close to 1, 
   thus further illustrating the tightness of the presented bounds in practice.

   (Joint work with Casper Freksen and Kasper Green Larsen)
---